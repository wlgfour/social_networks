#### Preamble ####
# Purpose: clean the data chunks that are generated by the scraping script.
#          combines chunks, removes duplicates, and scores messages
# Author: William Gerecke
# Email: wlgfour@gmail.com
# Date: 4/27/2022
# Prerequisites: R software
# Notes: this will take some time to run since it involves running Vader on
#        many different bodies of text
#        it will likely error out dur to a bug with garbage collection


# === data structure ===
# subreddit
# author
# id
# to_id
# permalink
# timestamp
# body



#install.packages('vader')
library(vader)
library(purrr)
library(dplyr)


# read all chunks
dfs <- map(list.files('outputs/raw', full.names=T), read.csv)
df <- bind_rows(dfs) |>
  distinct(id, to_id, .keep_all=T) |>
  select(!X)


# only include subreddits that have at least 500 comments
counts <- df |>
  group_by(subreddit) |>
  summarize(n=n()) |>
  filter(n > 500)
counts <- counts[order(counts$n, decreasing=T),]
barplot(counts$n, names.arg=counts$subreddit)
counts


# subset the comments gathered from subreddits
sub <- df |>
  filter(subreddit %in% counts$subreddit) |>
  group_by(subreddit) |>
  slice_sample(n=1000) |>
  ungroup()

counts <- sub |>
  group_by(subreddit) |>
  summarize(n=n()) |>
  filter(n > 500)
counts <- counts[order(counts$n, decreasing=T),]
barplot(counts$n, names.arg=counts$subreddit)

nrow(sub)


# process the data by chunks
splits <- split(sub, (as.numeric(rownames(sub))-1) %/% 200)
scored <- data.frame()
total <- 0
for (s in splits) {
  if (total > 10000 && total < 20000) next
  total <- total + nrow(s)
  print(paste(total, '/', nrow(sub), sep = ''))
  sco <- s |>
    mutate(
      sentiment=vader_df(body)$compound
    )
  scored <- rbind(scored, sco)
}

scored
scored |>
  filter(is.na(sentiment)) |>
  select(body)
  

write.csv(scored, 'outputs/processed/data.csv')







